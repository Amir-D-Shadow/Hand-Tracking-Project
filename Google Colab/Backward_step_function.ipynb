{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backward_step_function.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMspazX6Bdahv4w4DMAPvW4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amir-D-Shadow/Google-Colab/blob/main/Backward_step_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHXylrn9twdf"
      },
      "source": [
        "from numba import cuda,float64,int64\n",
        "import numpy as np\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBHysQrfqIZs"
      },
      "source": [
        "def zero_padding(img,padH,padW):\n",
        "\n",
        "        \"\"\"\n",
        "        img : numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "        pad : amount of padding around each image on vertical and horizontal dimensions\n",
        "        \"\"\"\n",
        "\n",
        "        return np.pad(img,((0,0),(padH,padH),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUQDIJshqbTp"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:],float64[:,:,:,:],float64[:,:,:],int64,int64,int64,int64\")\n",
        "def conv_step_forward3D(W,img,b,Z,stride,Hlim,Wlim,Clim):\n",
        "\n",
        "        \"\"\"\n",
        "        W -- (fH,fW,n_C_prev,Channels)\n",
        "        img -- (n_H_prev,n_W_prev,n_C_prev)\n",
        "        b -- (1,1,1,Channels)\n",
        "        Z -- (n_H,n_W,Channels)\n",
        "        \"\"\"\n",
        "\n",
        "        fH,fW,n_C_prev,_ = W.shape\n",
        "        n_H_prev,n_W_prev,n_C_prev = img.shape\n",
        "\n",
        "        n_H = cuda.threadIdx.x + cuda.blockIdx.x*cuda.blockDim.x\n",
        "        n_W = cuda.threadIdx.y + cuda.blockIdx.y*cuda.blockDim.y\n",
        "        n_C = cuda.threadIdx.z + cuda.blockIdx.z*cuda.blockDim.z\n",
        "\n",
        "        if (n_H < Hlim) and (n_W < Wlim) and (n_C < Clim):\n",
        "\n",
        "                #loop through height\n",
        "                for h in range(fH):\n",
        "\n",
        "                    #loop through width\n",
        "                    for w in range(fW):\n",
        "\n",
        "                        #loop through channels\n",
        "                        for c in range(n_C_prev):\n",
        "\n",
        "                            IMG_H = n_H*stride+h\n",
        "                            IMG_W = n_W*stride+w\n",
        "\n",
        "                            Z[n_H,n_W,n_C] = Z[n_H,n_W,n_C] + W[h,w,c,n_C]*img[IMG_H,IMG_W,c]\n",
        "\n",
        "                #wait until result come out\n",
        "                cuda.syncthreads()\n",
        "\n",
        "                #add bias\n",
        "                Z[n_H,n_W,n_C] = Z[n_H,n_W,n_C] + float(b[0,0,0,n_C])\n",
        "\n",
        "                #wait until result come out\n",
        "                cuda.syncthreads()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN2jFgiqqe8l"
      },
      "source": [
        "def Conv_Forward3D_GPU(A_prev,W,b,stride,padH=0,padW=0,padding=\"Valid\",threadsperblock=(8,8,8)):\n",
        "\n",
        "        \"\"\"\n",
        "        A_prev -- (m, n_H_prev, n_W_prev, n_C_prev) \n",
        "        W -- (fH, fW, n_C_prev, n_C) \n",
        "        b -- (1, 1, 1, n_C)\n",
        "        f -- kernel size\n",
        "        \"\"\"\n",
        "\n",
        "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "        fH, fW, n_C_prev, n_C = W.shape\n",
        "\n",
        "        #Padding\n",
        "        if padding == \"Same\":\n",
        "\n",
        "                A_prev_pad,opadH,opadW = same_padding(A_prev,stride,fH,fW)\n",
        "\n",
        "                n_H = n_H_prev\n",
        "                n_W = n_W_prev\n",
        "\n",
        "        elif padH != 0 or padW != 0:\n",
        "\n",
        "                A_prev_pad = zero_padding(A_prev,padH,padW)\n",
        "\n",
        "                n_H = int((n_H_prev+2*padH-fH)/stride)+1\n",
        "                n_W = int((n_W_prev+2*padW-fW)/stride)+1\n",
        "                opadH = padH\n",
        "                opadW = padW\n",
        "\n",
        "        else:\n",
        "\n",
        "                A_prev_pad = A_prev.copy()\n",
        "\n",
        "                n_H = int((n_H_prev-fH)/stride)+1\n",
        "                n_W = int((n_W_prev-fW)/stride)+1\n",
        "                opadH = 0\n",
        "                opadW = 0\n",
        "\n",
        "        #Set up\n",
        "        Z = np.zeros((m,n_H,n_W,n_C))\n",
        "\n",
        "        #Move memory to GPU\n",
        "        W_device = cuda.to_device(W)\n",
        "        b_device = cuda.to_device(b)\n",
        "\n",
        "        #Create stream\n",
        "        stream_list = []\n",
        "        for i in range(m):\n",
        "\n",
        "            stream_list.append(cuda.stream())\n",
        "\n",
        "        #define blocks for H and W\n",
        "        blockspergrid_H = int(math.ceil(n_H/threadsperblock[0]))\n",
        "        blockspergrid_W = int(math.ceil(n_W/threadsperblock[1]))\n",
        "        blockspergrid_C = int(math.ceil(n_C/threadsperblock[2]))\n",
        "\n",
        "        blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "        #cuda.synchronize()\n",
        "\n",
        "        #convolution\n",
        "        for s in range(m):\n",
        "\n",
        "            #Move memory to GPU    \n",
        "            Z_i = (Z[s,:,:,:]).copy()\n",
        "            Z_device = cuda.to_device(Z_i,stream = stream_list[s])\n",
        "            \n",
        "            A_prev_pad_i = (A_prev_pad[s,:,:,:]).copy()\n",
        "            A_prev_pad_device = cuda.to_device(A_prev_pad_i,stream = stream_list[s])\n",
        "            \n",
        "            #calculation\n",
        "            conv_step_forward3D[blockspergrid,threadsperblock,stream_list[s]](W_device,A_prev_pad_device,b_device,Z_device,stride,n_H,n_W,n_C)\n",
        "            cuda.synchronize()\n",
        "\n",
        "            #GET RESULT\n",
        "            Z[s,:,:,:] = Z_device.copy_to_host(stream = stream_list[i])\n",
        "            \n",
        "\n",
        "        #Save cache for back propagation\n",
        "        cacheL = (A_prev,W,b,stride,opadH,opadW)\n",
        "        cuda.synchronize()\n",
        "\n",
        "        return Z,cacheL"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2dTonNxzqHW"
      },
      "source": [
        "def conv_forward(A_prev,W,b,stride,padH=0,padW=0,padding=\"Valid\"):\n",
        "\n",
        "    \"\"\"\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (fH, fW, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    f -- kernel size\n",
        "    \"\"\"\n",
        "\n",
        "    #Retrieve Shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    (fH, fW, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    #Padding and determine the size of new layer \n",
        "\n",
        "    A_prev_pad = np.pad(A_prev,((0,0),(padH,padH),(padW,padW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "    \n",
        "    n_H = int((n_H_prev+2*padH-fH)/stride)+1\n",
        "    n_W = int((n_W_prev+2*padW-fW)/stride)+1\n",
        "\n",
        "    cpadH = padH\n",
        "    cpadW = padW\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    #Set up Z\n",
        "    Z = np.zeros((m,n_H,n_W,n_C))\n",
        "\n",
        "    #Convolute Forward\n",
        "    for i in range(m):\n",
        "\n",
        "        #Get a sample\n",
        "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
        "\n",
        "        #Loop over vertical axis \n",
        "        for h in range(n_H):\n",
        "\n",
        "              vert_start = h*stride\n",
        "              vert_end = vert_start + fH\n",
        "            \n",
        "              #Loop over horizontal axis\n",
        "              for w in range(n_W):\n",
        "\n",
        "                  hori_start = w*stride\n",
        "                  hori_end = hori_start + fW\n",
        "\n",
        "                  #Slice current sample\n",
        "                  a_slice_prev = a_prev_pad[vert_start:vert_end,hori_start:hori_end,:]\n",
        "\n",
        "                  #For each filter\n",
        "                  for c in range(n_C):\n",
        "\n",
        "                      Wc = W[:,:,:,c]\n",
        "                      bc = b[:,:,:,c]\n",
        "                        \n",
        "                      Z[i,h,w,c] =  np.sum(a_slice_prev*Wc)+float(bc)\n",
        "\n",
        "\n",
        "    cacheL = (A_prev,W,b,stride,cpadH,cpadW)\n",
        "\n",
        "    return Z,cacheL"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY4FOm7itCBV"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:,:],int64,int64,int64,int64\")\n",
        "def conv_step_backward3D_dW(A_prev_pad,dW,dZ,stride,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  A_prev_pad -- (m,n_H_prev,n_W_prev,n_C_prev)\n",
        "  dW -- (fH,fW,n_C_prev,segment_size)\n",
        "  dZ -- (m,n_H,n_W,segemnt_size)\n",
        "  \"\"\"\n",
        "\n",
        "  h = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  w = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  n_C_prev = cuda.threadIdx.z +cuda.blockDim.z*cuda.blockIdx.z\n",
        "\n",
        "  if (h < Hlim) and (w < Wlim) and (n_C_prev < Clim):\n",
        "\n",
        "    m,nH,nW,segemnt_size = dZ.shape\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      for n_h in range(nH):\n",
        "\n",
        "        for n_w in range(nW):\n",
        "\n",
        "            for n_c in range(segemnt_size):\n",
        "\n",
        "              IMG_H = n_h*stride + h\n",
        "              IMG_W = n_w*stride + w\n",
        "\n",
        "              dW[h,w,n_C_prev,n_c] = dW[h,w,n_C_prev,n_c] + A_prev_pad[i,IMG_H,IMG_W,n_C_prev]*dZ[i,n_h,n_w,n_c]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Yp34c-zMw4",
        "outputId": "0be171b0-b6aa-4faa-da9f-e6fbfa71bbf1"
      },
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10,4,4,3)\n",
        "W = np.random.randn(2,2,3,8)\n",
        "b = np.random.randn(1,1,1,8)\n",
        "opadH,opadW,stride = 2,2,2\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, stride,opadH,opadW)\n",
        "dZ = Z.copy()\n",
        "\n",
        "fH,fW,n_C_prev,n_C = W.shape \n",
        "\n",
        "threadsperblock = (4,4,4)\n",
        "\n",
        "segment_size = threadsperblock[-1]\n",
        "number_of_streams = int(math.ceil(n_C/segment_size))\n",
        "stream_list = []\n",
        "\n",
        "for i in range(number_of_streams):\n",
        "\n",
        "  stream_list.append(cuda.stream())\n",
        "\n",
        "blockspergrid_H = int(math.ceil(fH/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(fW/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "A_prev_pad = np.pad(A_prev,((0,0),(opadH,opadH),(opadW,opadW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "A_prev_pad_device = cuda.to_device(A_prev_pad)\n",
        "\n",
        "dW = np.zeros_like(W)\n",
        "\n",
        "for s in range(number_of_streams):\n",
        "\n",
        "  dW_device = cuda.to_device(dW[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "  dZ_device = cuda.to_device(dZ[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "\n",
        "  conv_step_backward3D_dW[blockspergrid,threadsperblock,stream_list[s]](A_prev_pad_device,dW_device,dZ_device,stride,fH,fW,n_C_prev)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  dW[:,:,:,(s*segment_size):((s+1)*segment_size)] = dW_device.copy_to_host(stream=stream_list[s])\n",
        "  cuda.synchronize()\n",
        "\n",
        "print(dW.mean())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7269914583139097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MHBlrygEKqZ"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],int64,int64,int64\")\n",
        "def conv_step_backward3D_db(db,dZ,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  db -- (1,1,1,n_C)\n",
        "  dZ -- (m,n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  x = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  y = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  z = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (x < Hlim) and (y < Wlim) and (z < Clim):\n",
        "\n",
        "    m,nH,nW,segemnt_size = dZ.shape\n",
        "\n",
        "    for i in range(m):\n",
        "\n",
        "      for n_h in range(nH):\n",
        "\n",
        "        for n_w in range(nW):\n",
        "\n",
        "          for n_c in range(segemnt_size):\n",
        "\n",
        "            db[x,y,z,n_c] = db[x,y,z,n_c] + dZ[i,n_h,n_w,n_c]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDoPSUVoJeej",
        "outputId": "906f73e1-667e-414e-c05f-99dc72905704"
      },
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10,4,4,3)\n",
        "W = np.random.randn(2,2,3,8)\n",
        "b = np.random.randn(1,1,1,8)\n",
        "opadH,opadW,stride = 2,2,2\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, stride,opadH,opadW)\n",
        "dZ = Z.copy()\n",
        "\n",
        "bH,bW,b_C_prev,n_C = b.shape \n",
        "\n",
        "threadsperblock = (2,2,2)\n",
        "\n",
        "segment_size = threadsperblock[-1]\n",
        "number_of_streams = int(math.ceil(n_C/segment_size))\n",
        "stream_list = []\n",
        "\n",
        "for i in range(number_of_streams):\n",
        "\n",
        "  stream_list.append(cuda.stream())\n",
        "\n",
        "blockspergrid_H = int(math.ceil(bH/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(bW/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(b_C_prev/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "db = np.zeros_like(b)\n",
        "\n",
        "for s in range(number_of_streams):\n",
        "\n",
        "  db_device = cuda.to_device(db[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "  dZ_device = cuda.to_device(dZ[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "\n",
        "  conv_step_backward3D_db[blockspergrid,threadsperblock,stream_list[s]](db_device,dZ_device,bH,bW,b_C_prev)\n",
        "  cuda.synchronize()\n",
        "\n",
        "  db[:,:,:,(s*segment_size):((s+1)*segment_size)] = db_device.copy_to_host(stream=stream_list[s])\n",
        "  cuda.synchronize()\n",
        "\n",
        "print(db.mean())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.839232564616838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcOyTwzKTeOe"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:,:],int64,int64,int64,int64\")\n",
        "def conv_step_backward3D_dA_prev_pad(dA_prev_pad,W,dZ,stride,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dA_prev_pad -- (m,n_H_prev,n_W_prev,n_C_prev)\n",
        "  W -- (fH,fW,n_C_prev,n_C)\n",
        "  dZ -- (m,n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  IMG_H = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  IMG_W = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  IMG_C_prev = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (IMG_H < Hlim) and (IMG_W < Wlim) and (IMG_C_prev < Clim):\n",
        "\n",
        "    fH,fW,n_C_prev,number_of_filters = W.shape\n",
        "    m,n_H,n_W,number_of_filters = dZ.shape\n",
        "\n",
        "    #method 1\n",
        "    #find the corresponding components\n",
        "    #loop through different example\n",
        "    for i in range(m):\n",
        "\n",
        "      #loop through different filters\n",
        "      for nc in range(number_of_filters):\n",
        "\n",
        "        for h in range(fH):\n",
        "\n",
        "          for w in range(fW):\n",
        "\n",
        "              nh = (IMG_H - h) / stride\n",
        "              nw = (IMG_W - w) / stride\n",
        "\n",
        "              if ((nh-int(nh)) == 0 ) and ((nw-int(nw)) == 0 ):\n",
        "\n",
        "                  #convert back to int as nh and nw become float after division\n",
        "                  nh = int(nh)\n",
        "                  nw = int(nw)\n",
        "\n",
        "                  dA_prev_pad[i,IMG_H,IMG_W,IMG_C_prev] = dA_prev_pad[i,IMG_H,IMG_W,IMG_C_prev] + W[h,w,IMG_C_prev,nc] * dZ[i,nh,nw,nc]      "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV-tEkQ1-tIo",
        "outputId": "d15a94fb-1b06-4e05-ce11-f9061c0eb4d2"
      },
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10,4,4,3)\n",
        "W = np.random.randn(2,2,3,8)\n",
        "b = np.random.randn(1,1,1,8)\n",
        "opadH,opadW,stride = 2,2,2\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, stride,opadH,opadW)\n",
        "dZ = Z.copy()\n",
        "\n",
        "m,n_H,n_W,n_C = dZ.shape\n",
        "\n",
        "dA_prev = np.zeros_like(A_prev)\n",
        "dA_prev_pad = np.pad(dA_prev,((0,0),(opadH,opadH),(opadW,opadW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "dA_prev_pad_device = cuda.to_device(dA_prev_pad)\n",
        "\n",
        "m,n_H_prev_pad,n_W_prev_pad,n_C_prev = dA_prev_pad.shape\n",
        "\n",
        "threadsperblock = (4,4,16)\n",
        "\n",
        "blockspergrid_H = int(math.ceil(n_H_prev_pad/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(n_W_prev_pad/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "segment_size = threadsperblock[-1]\n",
        "number_of_streams = int(math.ceil(n_C/segment_size))\n",
        "stream_list = []\n",
        "\n",
        "for i in range(number_of_streams):\n",
        "\n",
        "  stream_list.append(cuda.stream())\n",
        "\n",
        "for s in range(number_of_streams):\n",
        "\n",
        "  W_i = W[:,:,:,(s*segment_size):((s+1)*segment_size)].copy()\n",
        "  W_device = cuda.to_device(W_i,stream = stream_list[s])\n",
        "\n",
        "  dZ_i = dZ[:,:,:,(s*segment_size):((s+1)*segment_size)].copy()\n",
        "  dZ_device = cuda.to_device(dZ_i,stream = stream_list[s])\n",
        "\n",
        "  conv_step_backward3D_dA_prev_pad[blockspergrid,threadsperblock,stream_list[s]](dA_prev_pad_device,W_device,dZ_device,stride,n_H_prev_pad,n_W_prev_pad,n_C_prev)\n",
        "\n",
        "dA_prev_pad = dA_prev_pad_device.copy_to_host()\n",
        "dA_prev = dA_prev_pad[:,opadH:-opadH,opadW:-opadW,:]\n",
        "\n",
        "print(dA_prev.mean())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4524377775388075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w2A2cLPc2C7"
      },
      "source": [
        "@cuda.jit(\"float64[:,:,:,:],float64[:,:,:,:],float64[:,:,:,:],int64,int64,int64,int64\")\n",
        "def conv_step_backward3D_dA_prev_pad_test(dA_prev_pad,W,dZ,stride,Hlim,Wlim,Clim):\n",
        "\n",
        "  \"\"\"\n",
        "  dA_prev_pad -- (m,n_H_prev,n_W_prev,n_C_prev)\n",
        "  W -- (fH,fW,n_C_prev,n_C)\n",
        "  dZ -- (m,n_H,n_W,n_C)\n",
        "  \"\"\"\n",
        "\n",
        "  IMG_H = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
        "  IMG_W = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
        "  IMG_C_prev = cuda.threadIdx.z + cuda.blockDim.z * cuda.blockIdx.z\n",
        "\n",
        "  if (IMG_H < Hlim) and (IMG_W < Wlim) and (IMG_C_prev < Clim):\n",
        "\n",
        "    fH,fW,n_C_prev,number_of_filters = W.shape\n",
        "    m,n_H,n_W,number_of_filters = dZ.shape\n",
        "\n",
        "    #find the corresponding components\n",
        "    #loop through different example\n",
        "    for i in range(m):\n",
        "\n",
        "      #loop through different filters\n",
        "      for nc in range(number_of_filters):\n",
        "\n",
        "        for h in range(fH):\n",
        "\n",
        "          for w in range(fW):\n",
        "\n",
        "              nh = (IMG_H - h) / stride\n",
        "              nw = (IMG_W - w) / stride\n",
        "\n",
        "              if ((nh-int(nh)) == 0 ) and ((nw-int(nw)) == 0 ):\n",
        "\n",
        "                  nh = int(nh)\n",
        "                  nw = int(nw)\n",
        "\n",
        "                  dA_prev_pad[i,IMG_H,IMG_W,IMG_C_prev] = dA_prev_pad[i,IMG_H,IMG_W,IMG_C_prev] + W[h,w,IMG_C_prev,nc] * dZ[i,nh,nw,nc]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8IMvwqPc7Jx",
        "outputId": "22e58c20-69fc-403e-8bf4-4ee44743cf4d"
      },
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10,4,4,3)\n",
        "W = np.random.randn(2,2,3,8)\n",
        "b = np.random.randn(1,1,1,8)\n",
        "opadH,opadW,stride = 2,2,2\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, stride,opadH,opadW)\n",
        "dZ = Z.copy()\n",
        "\n",
        "m,n_H,n_W,n_C = dZ.shape\n",
        "\n",
        "dA_prev = np.zeros_like(A_prev)\n",
        "dA_prev_pad = np.pad(dA_prev,((0,0),(opadH,opadH),(opadW,opadW),(0,0)),mode=\"constant\",constant_values=(0,0))\n",
        "dA_prev_pad_device = cuda.to_device(dA_prev_pad)\n",
        "\n",
        "m,n_H_prev_pad,n_W_prev_pad,n_C_prev = dA_prev_pad.shape\n",
        "\n",
        "threadsperblock = (4,4,16)\n",
        "\n",
        "blockspergrid_H = int(math.ceil(n_H_prev_pad/threadsperblock[0]))\n",
        "blockspergrid_W = int(math.ceil(n_W_prev_pad/threadsperblock[1]))\n",
        "blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "segment_size = threadsperblock[-1]\n",
        "number_of_streams = int(math.ceil(n_C/segment_size))\n",
        "stream_list = []\n",
        "\n",
        "for i in range(number_of_streams):\n",
        "\n",
        "  stream_list.append(cuda.stream())\n",
        "\n",
        "for s in range(number_of_streams):\n",
        "\n",
        "  W_i = W[:,:,:,(s*segment_size):((s+1)*segment_size)].copy()\n",
        "  W_device = cuda.to_device(W_i,stream = stream_list[s])\n",
        "\n",
        "  dZ_i = dZ[:,:,:,(s*segment_size):((s+1)*segment_size)].copy()\n",
        "  dZ_device = cuda.to_device(dZ_i,stream = stream_list[s])\n",
        "\n",
        "  conv_step_backward3D_dA_prev_pad_test[blockspergrid,threadsperblock,stream_list[s]](dA_prev_pad_device,W_device,dZ_device,stride,n_H_prev_pad,n_W_prev_pad,n_C_prev)\n",
        "\n",
        "dA_prev_pad = dA_prev_pad_device.copy_to_host()\n",
        "dA_prev = dA_prev_pad[:,opadH:-opadH,opadW:-opadW,:]\n",
        "\n",
        "print(dA_prev.mean())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4524377775388075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l9LJiwkpkkb"
      },
      "source": [
        "#ConvBackward\n",
        "def Conv_backward3D_GPU(dZ,cacheL,threadsperblock=(8,8,8)):\n",
        "\n",
        "        \"\"\"\n",
        "        dA -- (m,n_H,n_W,n_C)\n",
        "        cacheL -- (A_prev,W,b,stride,opadH,opadW)\n",
        "        \"\"\"\n",
        "        #Get informaton from cacheL\n",
        "        A_prev,W,b,stride,opadH,opadW = cacheL\n",
        "\n",
        "        #Get the shape of prev layer\n",
        "        m,n_H_prev,n_W_prev,n_C_prev = A_prev.shape\n",
        "        \n",
        "        #Get the shape of current layer\n",
        "        m,n_H,n_W,n_C = dZ.shape\n",
        "\n",
        "        #Get the shape of filter\n",
        "        fH, fW, n_C_prev,n_C = W.shape\n",
        "\n",
        "        #Get the shape of b\n",
        "        bH,bW,b_C_prev,n_C = b.shape\n",
        "        \n",
        "        #Set up\n",
        "        dA_prev = np.zeros((m,n_H_prev,n_W_prev,n_C_prev))\n",
        "        dW = np.zeros((fH,fW,n_C_prev,n_C))\n",
        "        db = np.zeros((1,1,1,n_C))\n",
        "\n",
        "        #move memory to GPU A_prev_pad,dA_prev_pad\n",
        "        if opadH == 0 and opadW == 0:\n",
        "\n",
        "                #A_prev -- (m,n_H_prev,n_W_prev,n_C_prev)\n",
        "                A_prev_pad = A_prev.copy()\n",
        "                #dA_prev -- (m,n_H_prev,n_W_prev,n_C_prev)\n",
        "                dA_prev_pad = dA_prev.copy()\n",
        "        else:\n",
        "                \n",
        "                A_prev_pad = zero_padding(A_prev,opadH,opadW)\n",
        "                dA_prev_pad = zero_padding(dA_prev,opadH,opadW)\n",
        "\n",
        "        A_prev_pad_device = cuda.to_device(A_prev_pad)\n",
        "        dA_prev_pad_device = cuda.to_device(dA_prev_pad)\n",
        "\n",
        "        #define stream \n",
        "        segment_size = threadsperblock[-1]\n",
        "        number_of_streams = int(math.ceil(n_C/segment_size))\n",
        "\n",
        "        stream_list = []\n",
        "        for i in range(number_of_streams):\n",
        "\n",
        "                stream_list.append(cuda.stream())\n",
        "                \n",
        "        \n",
        "        #blockspergrid for dA_prev_pad\n",
        "        m,n_H_prev_pad,n_W_prev_pad,n_C_prev = dA_prev_pad.shape\n",
        "        \n",
        "        blockspergrid_H = int(math.ceil(n_H_prev_pad/threadsperblock[0]))\n",
        "        blockspergrid_W = int(math.ceil(n_W_prev_pad/threadsperblock[1]))\n",
        "        blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "        blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "       \n",
        "        #backward dA_prev_pad\n",
        "        for s in range(number_of_streams):\n",
        "\n",
        "                #move memory to GPU\n",
        "                W_device = cuda.to_device((W[:,:,:,(s*segment_size):((s+1)*segment_size)]).copy(),stream = stream_list[s])                \n",
        "                dZ_device = cuda.to_device((dZ[:,:,:,(s*segment_size):((s+1)*segment_size)]).copy(),stream = stream_list[s])\n",
        "\n",
        "                #calculation\n",
        "                conv_step_backward3D_dA_prev_pad[blockspergrid,threadsperblock,stream_list[s]](dA_prev_pad_device,W_device,dZ_device,stride,n_H_prev_pad,n_W_prev_pad,n_C_prev)\n",
        "                \n",
        "        \n",
        "        #Get Result dA_prev\n",
        "        cuda.synchronize()\n",
        "        dA_prev_pad = dA_prev_pad_device.copy_to_host()\n",
        "        \n",
        "        \"\"\"\n",
        "        #calculate dA_prev_pad\n",
        "        conv_step_backward3D_dA_prev_pad_jit(dA_prev_pad,W,dZ,stride)\n",
        "        \"\"\"\n",
        "        \n",
        "        if opadH != 0 and opadW != 0:\n",
        "\n",
        "                dA_prev[:,:,:,:] = dA_prev_pad[:,opadH:-opadH,opadW:-opadW,:]\n",
        "\n",
        "        elif opadH != 0 and opadW == 0:\n",
        "\n",
        "                dA_prev[:,:,:,:] = dA_prev_pad[:,opadH:-opadH,:,:]\n",
        "\n",
        "        elif opadH ==0 and opadW != 0:\n",
        "\n",
        "                dA_prev[:,:,:,:] = dA_prev_pad[:,:,opadW:-opadW,:]\n",
        "\n",
        "        else:\n",
        "                dA_prev = dA_prev_pad\n",
        "\n",
        "        #blockspergrid for dW\n",
        "        blockspergrid_H = int(math.ceil(fH/threadsperblock[0]))\n",
        "        blockspergrid_W = int(math.ceil(fW/threadsperblock[1]))\n",
        "        blockspergrid_C = int(math.ceil(n_C_prev/threadsperblock[2]))\n",
        "\n",
        "        blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "        #backward dW\n",
        "        for s in range(number_of_streams):\n",
        "\n",
        "          dW_device = cuda.to_device(dW[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "          dZ_device = cuda.to_device(dZ[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "\n",
        "          conv_step_backward3D_dW[blockspergrid,threadsperblock,stream_list[s]](A_prev_pad_device,dW_device,dZ_device,stride,fH,fW,n_C_prev)\n",
        "          cuda.synchronize()\n",
        "\n",
        "          dW[:,:,:,(s*segment_size):((s+1)*segment_size)] = dW_device.copy_to_host(stream=stream_list[s])\n",
        "\n",
        "        #blockspergrid for db\n",
        "        blockspergrid_H = int(math.ceil(bH/threadsperblock[0]))\n",
        "        blockspergrid_W = int(math.ceil(bW/threadsperblock[1]))\n",
        "        blockspergrid_C = int(math.ceil(b_C_prev/threadsperblock[2]))\n",
        "\n",
        "        blockspergrid = (blockspergrid_H,blockspergrid_W,blockspergrid_C)\n",
        "\n",
        "        cuda.synchronize()\n",
        "        \n",
        "        #backward db\n",
        "        for s in range(number_of_streams):\n",
        "\n",
        "          db_device = cuda.to_device(db[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "          dZ_device = cuda.to_device(dZ[:,:,:,(s*segment_size):((s+1)*segment_size)].copy(),stream=stream_list[s])\n",
        "\n",
        "          conv_step_backward3D_db[blockspergrid,threadsperblock,stream_list[s]](db_device,dZ_device,bH,bW,b_C_prev)\n",
        "          cuda.synchronize()\n",
        "\n",
        "          db[:,:,:,(s*segment_size):((s+1)*segment_size)] = db_device.copy_to_host(stream=stream_list[s])\n",
        "\n",
        "        cuda.synchronize()\n",
        "\n",
        "        return dA_prev,dW,db"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWvhXQHPqNw-",
        "outputId": "c64cbe24-dd77-4704-bf15-4a417aa19ca4"
      },
      "source": [
        "np.random.seed(1)\n",
        "img = np.random.randn(10,1200,1200,3)\n",
        "W = np.random.randn(3,3,3,64)\n",
        "b = np.random.randn(1,1,1,64)\n",
        "stride = 2\n",
        "\n",
        "Z,cacheL = Conv_Forward3D_GPU(img,W,b,stride,padH=2,padW=2,threadsperblock=(8,8,8))\n",
        "cuda.synchronize()\n",
        "\n",
        "m,n_H,n_W,n_C = Z.shape\n",
        "gpu_time = time.time()\n",
        "dA_prev,dW,db = Conv_backward3D_GPU(Z,cacheL,threadsperblock=(8,8,8))#obj.conv_backward(Z, cacheL,lambda x:x)\n",
        "cuda.synchronize()\n",
        "print(f\"With GPU:{time.time()-gpu_time}\")\n",
        "print(\"\\n\")\n",
        "print(\"dA_mean =\", np.mean(dA_prev))\n",
        "print(\"dW_mean =\", np.mean(dW))\n",
        "print(\"db_mean =\", np.mean(db))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With GPU:102.0307137966156\n",
            "\n",
            "\n",
            "dA_mean = -2.5731674455493065\n",
            "dW_mean = -117957.5402107956\n",
            "db_mean = -76630.87028957192\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}